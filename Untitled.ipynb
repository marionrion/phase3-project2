{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pump it up-Data Mining the Water Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining the Problem\n",
    "Tanzania faces a major water crisis. Millions lack access to clean drinking water due to limited freshwater sources and malfunctioning water points. This lack of safe water has severe consequences, including health risks, decreased quality of life, and even death for children. The Tanzanian government is working to improve sanitation, but better water resource management is crucial for the country's future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1Tanzania faces a critical challenge:\n",
    "Millions of people lack access to safe drinking water due to malfunctioning water points. This project aims to leverage machine learning to predict the functionality of water points, helping prioritize maintenance and ensure clean water reaches communities across the country. By analyzing data on factors like pump type, installation age, location, and management practices, we can build a model to classify water points into three categories: functional, needing repair, or non-functional. This information can empower authorities to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2Target Maintenance Efforts:\n",
    "Prioritize repairs for water points most at risk of failure, ensuring efficient resource allocation and minimizing downtime. Preventative Maintenance: Identify pumps nearing the end of their lifespan or susceptible to breakdowns based on historical data, prompting proactive maintenance to avoid service disruptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3Improve Resource Management:\n",
    "Gain insights into factors affecting water point functionality, informing strategies for pump selection, installation practices, and long-term management approaches.\n",
    "By harnessing the power of machine learning, we can move beyond reactive repairs and towards a proactive approach to ensuring clean water security for Tanzania's population. This project tackles the critical business problem of water scarcity by predicting water point functionality, ultimately contributing to improved public health and well-being.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Our project will be successful if we can accurately predict whether a water point is:\n",
    "a. Fully Functional: The water point is operational and delivers clean water without any current repairs needed.\n",
    "\n",
    "b. Partially Functional (Needs Repair): The water point is currently operational, but there are potential issues requiring repairs to ensure continued functionality.\n",
    "\n",
    "c. Non-Functional: The water point is completely out of service and requires repairs to provide clean water again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _qhull: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-26a4b7838d6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m  \u001b[1;31m# For advanced data visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m    \u001b[1;31m# For data manipulation and analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m     \u001b[1;31m# For numerical computations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import seaborn objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\palettes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhusl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdesaturate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_color_cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxkcd_rgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrayons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\seaborn\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    483\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_measurements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m from scipy._lib._util import (check_random_state, MapWrapper,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\spatial\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_qhull\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_spherical_voronoi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSphericalVoronoi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_plotutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _qhull: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Importing necessary packages for data analysis, visualization, and machine learning\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns  # For advanced data visualization\n",
    "import pandas as pd    # For data manipulation and analysis\n",
    "import numpy as np     # For numerical computations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.svm import LinearSVC  # For linear support vector machines\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  # For hyperparameter tuning\n",
    "from sklearn.pipeline import Pipeline   # For creating a pipeline of data processing and models\n",
    "from sklearn.preprocessing import StandardScaler  # For data standardization\n",
    "from sklearn.ensemble import RandomForestClassifier  # For random forest classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # For gradient boosting classification\n",
    "from datetime import datetime  \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "train = pd.read_csv('test doc.csv')\n",
    "test = pd.read_csv('test doc 2.csv')\n",
    "data = pd.read_csv('test doc 3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.merge(data,on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_data, test])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Understanding the columns of the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of data points : ', df.shape[0])\n",
    "print('Number of features : ', df.shape[1])\n",
    "print('Features : ', df.columns.values)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=100 #for reading all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for colum names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_counts(data):\n",
    "  for column in data.columns:\n",
    "    print(f'value counts for {column}')\n",
    "    print(data[column].value_counts())\n",
    "    print('------------------------------------------','\\n')\n",
    "\n",
    "check_value_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data doesn't have any data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows the distribution of missing values across different features. Some features have a significant number of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample of records to see whether all records are appropiately ordered\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    construction_year=df['construction_year'].fillna(1993),\n",
    "    age=pd.to_datetime(df['date_recorded']).dt.year - df['construction_year'],\n",
    "    pop_year=df['population'].replace({0: 1}) / (df['construction_year'].fillna(1993) - pd.to_datetime(df['date_recorded']).dt.year).clip(lower=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we check for missing values \n",
    "# Dealing with missing values \n",
    "# Checking the mumber of missing values by column and sorting for the smallest\n",
    "\n",
    "Total = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Calculating percentages\n",
    "percent_1 = df.isnull().sum()/df.isnull().count()*100\n",
    "\n",
    "# rounding off to one decimal point\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "\n",
    "# creating a dataframe to show the values\n",
    "missing_data = pd.concat([Total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data seems to have a significant amount of missing values in some key columns, particularly scheme_name and status_group which could impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(\"Not Available\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing(value):\n",
    "  if pd.isna(value):\n",
    "    return \"Missing Value\"\n",
    "  else:\n",
    "    return value\n",
    "\n",
    "df = df.fillna(replace_missing)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values (inplace)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in 'scheme_name' with \"No Record\"\n",
    "df.scheme_name = df.scheme_name.fillna('No Record')\n",
    "\n",
    "# Impute missing values in 'scheme_management' with \"No Record\"\n",
    "df.scheme_management = df.scheme_management.fillna('No Record')\n",
    "\n",
    "# Impute missing values in 'installer' with \"No Record\"\n",
    "df.installer = df.installer.fillna('No Record')\n",
    "\n",
    "# Impute missing values in 'funder' with \"No Record\"\n",
    "df.funder = df.funder.fillna('No Record')\n",
    "\n",
    "# Impute missing values in 'public_meeting' with \"No Record\" (might need adjustment)\n",
    "df.public_meeting = df.public_meeting.fillna('No Record')  # Consider 'Unknown' if data could be missing but meetings happened\n",
    "\n",
    "# Impute missing values in 'permit' with \"No Record\"\n",
    "df.permit = df.permit.fillna('No Record')\n",
    "\n",
    "# Impute missing values in 'subvillage' with \"No Record\"\n",
    "df.subvillage = df.subvillage.fillna('No Record')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is now clean and ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get a brief description of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic descriptive statistics\n",
    "df['status_group'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This output shows the distribution of water point statuses in the data.\n",
    "# There are 4 unique statuses, with 'functional' being the most frequent (occurring 32259 times, or approximately 43.4% of the data).\n",
    "# Further analysis can be done to understand the prevalence of other statuses and their implications for water point functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique values and their counts\n",
    "\n",
    "n_unique_values = df['status_group'].nunique()  # Get the number of unique values\n",
    "value_counts = df['status_group'].value_counts()  # Get the count for each unique value\n",
    "\n",
    "print(f\"Number of unique values: {n_unique_values}\")\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This output shows the distribution of water point statuses in the data.\n",
    "# There are 4 unique categories describing water point functionality:\n",
    "#   * 'functional': Represents water points that are operational (32259 counts).\n",
    "#   * 'non functional': Represents water points that are not functioning (22824 counts).\n",
    "#   * '<function replace_missing at 0x000001561A526790>': This likely represents a placeholder value used during data cleaning \n",
    "#     (14850 counts). It's recommended to investigate and replace it with a more informative category (e.g., 'missing').\n",
    "#   * 'functional needs repair': Represents water points requiring maintenance (4317 counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis using groupby functionality\n",
    "unique_value_counts = df.groupby('status_group').size()  # Group by status and count occurrences\n",
    "print(unique_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting object datatypes columns\n",
    "\n",
    "# Create a list to store categorical column names\n",
    "categorical = [\n",
    "    # Columns containing text or non-numerical data\n",
    "    'basin', 'region',\n",
    "    'public_meeting', 'recorded_by',\n",
    "    'scheme_management', 'permit',\n",
    "    'extraction_type_group', 'extraction_type_class',\n",
    "    'management', 'management_group', 'payment_type',\n",
    "    'quality_group', 'quantity_group',\n",
    "    'source', 'source_type', 'source_class',\n",
    "    'waterpoint_type_group'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_meeting_counts = df['public_meeting'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "public_meeting_counts.plot(kind='bar', color=['skyblue', 'lightgreen'])\n",
    "plt.xlabel('Public Meeting Held')\n",
    "plt.ylabel('Number of Water Points')\n",
    "plt.title('Frequency of Public Meetings Held')\n",
    "plt.xticks(rotation=0)  # Ensure all labels are visible\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  # Import seaborn for boxplots\n",
    "\n",
    "recorded_by_counts = df['recorded_by'].value_counts()  # Get value counts\n",
    "\n",
    "# Assuming 'recorded_by' is a categorical variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=recorded_by_counts.index, y=recorded_by_counts.values)  # Use counts as y-axis values\n",
    "plt.xlabel('Recorded By')\n",
    "plt.ylabel('Number of Water Points')\n",
    "plt.title('Distribution of Recorded By (Boxplot)')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='scheme_management', data=df)\n",
    "plt.xlabel('Scheme Management')\n",
    "plt.ylabel('Number of Water Points')\n",
    "plt.title('Water Point Management Distribution')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histogram(data, categorical_column):\n",
    "  \"\"\"\n",
    "  This function creates a histogram for a categorical variable.\n",
    "\n",
    "  Args:\n",
    "    data: The pandas DataFrame containing the data.\n",
    "    categorical_column: The name of the categorical column to visualize.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(8, 5))  # Adjust figure size as needed\n",
    "  plt.hist(data[categorical_column])\n",
    "  plt.xlabel(categorical_column)\n",
    "  plt.ylabel('Number of Water Points')\n",
    "  plt.title(f\"Histogram of {categorical_column}\", fontsize=12)\n",
    "  plt.xticks(rotation=45)  # Rotate x-axis labels for readability (optional)\n",
    "  plt.show()\n",
    "\n",
    "# Example usage with slight modification (assuming 'basin' is a categorical column):\n",
    "draw_histogram(df, 'basin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " shows the distribution of construction year across different basin categories. It helps identify if construction years vary significantly between basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histogram(data, categorical_column):\n",
    "  \"\"\"\n",
    "  This function creates a histogram for a categorical feature.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the data.\n",
    "      categorical_column: The name of the categorical column to visualize.\n",
    "  \"\"\"\n",
    "  sns.histplot(\n",
    "      x = categorical_column,\n",
    "      data=df,\n",
    "      multiple='dodge'  # Dodge bars to avoid overlapping for multiple categories\n",
    "  )\n",
    "  plt.title(f\"Histogram of {categorical_column}\", fontsize=12)\n",
    "  plt.xticks(rotation=45)  # Rotate x-axis labels for readability (optional)\n",
    "  plt.show()\n",
    "  plt.clf()  # Clear the plot\n",
    "\n",
    "# Example usage:\n",
    "draw_histogram(df, 'region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bar_chart(data, categorical_column):\n",
    "  \"\"\"\n",
    "  This function creates a bar chart for a categorical feature with counts.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the data.\n",
    "      categorical_column: The name of the categorical column to visualize.\n",
    "  \"\"\"\n",
    "  data[categorical_column].value_counts().plot(kind='bar')\n",
    "  plt.title(f\"Count of Public Meetings ({categorical_column})\", fontsize=12)\n",
    "  plt.xlabel(categorical_column)\n",
    "  plt.ylabel('Count')\n",
    "  plt.xticks(rotation=0)  # Keep x-axis labels horizontal\n",
    "  plt.show()\n",
    "  plt.clf()  # Clear the plot\n",
    "\n",
    "# Example usage:\n",
    "draw_bar_chart(df.copy(), 'public_meeting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart shows the number of water points with and without public meetings. It helps understand the prevalence of public meetings for water point projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bar_chart(data, categorical_column):\n",
    "  \"\"\"\n",
    "  This function creates a bar chart for a categorical feature with counts.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the data.\n",
    "      categorical_column: The name of the categorical column to visualize.\n",
    "  \"\"\"\n",
    "  data[categorical_column].value_counts().plot(kind='bar')\n",
    "  plt.title(f\"Count of Permits ({categorical_column})\", fontsize=12)\n",
    "  plt.xlabel(categorical_column)\n",
    "  plt.ylabel('Count')\n",
    "  plt.xticks(rotation=0)  # Keep x-axis labels horizontal\n",
    "  plt.show()\n",
    "  plt.clf()  # Clear the plot\n",
    "\n",
    "# Example usage:\n",
    "draw_bar_chart(df.copy(), 'permit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart shows the number of water points with and without permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxplot(data, categorical_column, target_column):\n",
    "  \"\"\"\n",
    "  This function creates a boxplot for a categorical feature vs a target variable.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the data.\n",
    "      categorical_column: The name of the categorical column to visualize.\n",
    "      target_column: The name of the target numerical column (e.g., construction_year).\n",
    "  \"\"\"\n",
    "  sns.boxplot(\n",
    "      x = categorical_column,\n",
    "      y = 'construction_year',  # Replace with your target numerical variable if needed\n",
    "      showmeans=True,\n",
    "      data=data\n",
    "  )\n",
    "  plt.title(f\"Boxplot of {categorical_column} vs Construction Year\", fontsize=12)\n",
    "  plt.xticks(rotation=45)  # Rotate x-axis labels for readability (optional)\n",
    "  plt.show()\n",
    "  plt.clf()  # Clear the plot\n",
    "\n",
    "# Example usage:\n",
    "draw_boxplot(df.copy(), 'extraction_type_group', 'construction_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boxplot shows the distribution of construction year across different extraction type groups. It helps identify if construction years vary significantly between extraction types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histogram(data, categorical_column):\n",
    "  \"\"\"\n",
    "  This function creates a histogram for a categorical feature.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the data.\n",
    "      categorical_column: The name of the categorical column to visualize.\n",
    "  \"\"\"\n",
    "  sns.histplot(\n",
    "      x = categorical_column,\n",
    "      data=df,\n",
    "      multiple='dodge'  # Dodge bars to avoid overlapping for multiple categories\n",
    "  )\n",
    "  plt.title(f\"Histogram of {categorical_column}\", fontsize=12)\n",
    "  plt.xticks(rotation=45)  # Rotate x-axis labels for readability (optional)\n",
    "  plt.show()\n",
    "  plt.clf()  # Clear the plot\n",
    "\n",
    "# Example usage:\n",
    "draw_histogram(df.copy(), 'extraction_type_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the frequency of each extraction type class in your data. It helps visualize the distribution of water points across different extraction type details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line_chart(data, categorical_column, target_column):\n",
    "  \"\"\"\n",
    "  This function creates a line chart for a categorical feature vs a numerical variable.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing the data.\n",
    "      categorical_column: The name of the categorical column to visualize on x-axis.\n",
    "      target_column: The name of the target numerical column (e.g., construction_year).\n",
    "  \"\"\"\n",
    "  data.groupby(categorical_column)[target_column].mean().plot(kind='line')\n",
    "  plt.title(f\"Average {target_column} by {categorical_column}\", fontsize=12)\n",
    "  plt.xlabel(categorical_column)\n",
    "  plt.ylabel(target_column)\n",
    "  plt.xticks(rotation=45)  # Rotate x-axis labels for readability (optional)\n",
    "  plt.show()\n",
    "  plt.clf()  # Clear the plot\n",
    "\n",
    "# Example usage:\n",
    "draw_line_chart(df.copy(), 'management', 'construction_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line chart shows the average construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(20,20));# distribution of numerical predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the scatter plots of predictor variables against the target variable suggest a balanced distribution across predictor values, further analysis is needed to understand the relationships between these variables. This will help determine their suitability for building a robust prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(data, columns):\n",
    "    fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(20,10))\n",
    "    for i, column in enumerate(columns):\n",
    "        # Use interquartile range (IQR) to find outliers for the specified column\n",
    "        q1 = data[column].quantile(0.25)\n",
    "        q3 = data[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        print(\"IQR for {} column: {}\".format(column, iqr))\n",
    "\n",
    "        # Determine the outliers based on the IQR\n",
    "        outliers = (data[column] < q1 - 1.5 * iqr) | (data[column] > q3 + 1.5 * iqr)\n",
    "        print(\"Number of outliers in {} column: {}\".format(column, outliers.sum()))\n",
    "\n",
    "        # Create a box plot to visualize the distribution of the specified column\n",
    "        sns.boxplot(data=data, x=column, ax=axes[i])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "num=df.select_dtypes('number')\n",
    "columns=num.columns\n",
    "check_outliers(df, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has outliers but we won't remove them because that information could be useful for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the status-group\n",
    "\n",
    "df_status=df[['id', 'status_group']]\n",
    "df_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_features=df[['id','amount_tsh', 'date_recorded', 'funder',\n",
    "       'gps_height', 'installer', 'longitude', 'latitude', 'wpt_name',\n",
    "       'num_private', 'basin', 'subvillage', 'region', 'region_code',\n",
    "       'district_code', 'lga', 'ward', 'population', 'public_meeting',\n",
    "       'recorded_by', 'scheme_management', 'scheme_name', 'permit',\n",
    "       'construction_year', 'extraction_type', 'extraction_type_group',\n",
    "       'extraction_type_class', 'management', 'management_group', 'payment',\n",
    "       'payment_type', 'water_quality', 'quality_group', 'quantity',\n",
    "       'quantity_group', 'source', 'source_type', 'source_class',\n",
    "       'waterpoint_type', 'waterpoint_type_group', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_status.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check data types\n",
    "print(df_status.dtypes)\n",
    "\n",
    "# If there are functions or other non-string values\n",
    "non_strings = df_status['status_group'].apply(lambda x: not isinstance(x, str))\n",
    "df_status = df_status[~non_strings]  # Filter out rows with non-strings\n",
    "\n",
    "# Now you can use LabelEncoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df_status['status_group'] = label_encoder.fit_transform(df_status['status_group'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'status_group'.\n",
    "df_status['status_group'] = label_encoder.fit_transform(df_status['status_group'])\n",
    "\n",
    "df_status['status_group'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df\n",
    "df['water_/_person'] = df['amount_tsh'].replace({0:1}) / df['population'].replace({0:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then write a function to check for the cardinality of each feature(how many unique values there are in the feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_cardinality_check(n, df):\n",
    "\n",
    "# this function will search the dataframe for features above the cardinality limit, \n",
    "# then create a dict from the results\n",
    "\n",
    "  \n",
    "  feature_list = []\n",
    "  \n",
    "  cardinality_value = []\n",
    "  \n",
    "  for _ in range(len(df.columns)):\n",
    "    if len(df[df.columns[_]].value_counts()) > n:\n",
    "      \n",
    "      feature_list.append(df.columns[_])\n",
    "      \n",
    "      cardinality_value.append(len(df[df.columns[_]].value_counts()))\n",
    "                               \n",
    "        \n",
    "  feature_dict = dict(zip(feature_list, cardinality_value))\n",
    "  \n",
    "  return feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then preview our high cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_feature_dict = reverse_cardinality_check(150, df)\n",
    "high_cardinality_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create dataframes for our high and low cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for high cardinality\n",
    "high_cardinality_features = df[list(high_cardinality_feature_dict.keys())]\n",
    "high_cardinality_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for low cardinality features\n",
    "low_cardinality_features = df.drop(columns = list(high_cardinality_feature_dict.keys()))\n",
    "low_cardinality_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform one hot encoding for each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "  \"\"\"\n",
    "  This function cleans data by handling missing values and converting booleans to strings.\n",
    "\n",
    "  Args:\n",
    "      data: A pandas DataFrame.\n",
    "\n",
    "  Returns:\n",
    "      A cleaned pandas DataFrame.\n",
    "  \"\"\"\n",
    "  # Fill missing values with a placeholder (e.g., -1 or a string)\n",
    "  data = data.fillna(-1)  # Replace with your preferred method\n",
    "\n",
    "  # Remove rows with functions or other non-string values (excluding missing values)\n",
    "  non_strings = data.apply(lambda x: not isinstance(x, str))\n",
    "  clean_data = data[~(non_strings.any())]  # Use .any() to combine conditions\n",
    "\n",
    "  # Convert booleans to strings (optional)\n",
    "  clean_data = clean_data.applymap(lambda x: str(x) if isinstance(x, bool) else x)\n",
    "  return clean_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = low_cardinality_features.concat(high_cardinality_features,on = low_cardinality_features.index)\n",
    "frames =[low_cardinality_features, high_cardinality_features]\n",
    "\n",
    "features = pd.concat(frames, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previewing the datatset\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we impute and scale our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging df_status and features\n",
    "df_1 = df_status.merge(features, left_on='id', right_on='id')\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test are different shapes. Find which columns are different.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df_1' is your DataFrame\n",
    "\n",
    "# Preprocessing (replace with your data loading steps)\n",
    "# ... (load your data into df_1)\n",
    "\n",
    "# Define y variable (dependent variable) - indicates tap functionality\n",
    "y = df['waterpoint_type_group'] == 'functional'  # Binary indicator\n",
    "\n",
    "# Define x variable (independent variable) - explore functionality by region\n",
    "x = df['region']\n",
    "\n",
    "# Split data into training and testing sets (assuming scikit-learn is installed)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ... (Train your machine learning model using X_train and y_train)\n",
    "\n",
    "# ... (Evaluate your model performance on X_test and y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df: ', X_train.shape, y_train.shape)\n",
    "print('df: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train is a Series\n",
    "X_train = pd.DataFrame(X_train)  # Convert Series to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming you've loaded your data into X_train\n",
    "\n",
    "# Check if X_train is a DataFrame (optional)\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train = pd.DataFrame(X_train)  # Convert Series to DataFrame (if needed)\n",
    "\n",
    "# Check for column presence\n",
    "if 'Region' in X_train.columns:\n",
    "    # Check for missing values\n",
    "    missing_values = X_train['Region'].isnull().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Warning: {missing_values} missing values in 'Region' column.\")\n",
    "        # Handle missing values (e.g., imputation or dropping rows)\n",
    "\n",
    "    # Proceed with label encoding\n",
    "    le = LabelEncoder()\n",
    "    X_train['Region'] = le.fit_transform(X_train['Region'])  # Assuming 'Region' is the categorical column\n",
    "else:\n",
    "    print(\"Error: 'Region' column not found in X_train data.\")\n",
    "    # Handle the missing column (e.g., investigate data source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if X_train is a DataFrame (optional)\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train = pd.DataFrame(X_train)  # Convert Series to DataFrame (if needed)\n",
    "\n",
    "# Assuming you have region data in a separate list or variable called 'region_data'\n",
    "X_train['Region'] = region_data  # Add the new column\n",
    "\n",
    "# Now X_train will have the 'Region' column with your region data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Region'] = ['Unknown'] * len(X_train)  # Fill with 'Unknown' for all rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is in a DataFrame called 'df'\n",
    "\n",
    "# Iterate over the columns of the dataframe\n",
    "for col in df.columns:\n",
    "  # Check if the data type of the column is float\n",
    "  if pd.api.types.is_float_dtype(df[col]):\n",
    "    # Convert the column to string type with 'f' format specifier to maintain precision (optional)\n",
    "    df[col] = df[col].astype(str)\n",
    "  else:\n",
    "    # Leave non-float columns unchanged\n",
    "    pass\n",
    "\n",
    "# Now 'df' will have all float columns converted to strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(col):\n",
    "  if pd.api.types.is_float_dtype(col):\n",
    "    return col.astype(str)\n",
    "  else:\n",
    "    return col\n",
    "\n",
    "df = df.apply(convert_to_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the algorithm with k=4 neighbors at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the training data (assuming 'region' is the column containing categorical values)\n",
    "X_train['region'] = le.fit_transform(X_train['region'])\n",
    "\n",
    "# Now X_train['region'] will contain integer labels for each region (e.g., 0 for \"Kagera\")\n",
    "\n",
    "# Repeat the encoding for the testing data (X_test) using the fitted encoder (le)\n",
    "X_test['region'] = le.transform(X_test['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "filename = 'pumptree.png'\n",
    "featureNames = df.columns[0:196]\n",
    "targetNames = df['status_group']\n",
    "out = tree.export_graphviz(dtc, feature_names=featureNames, \n",
    "                           out_file=dot_data, \n",
    "                           class_names=np.unique(y_train), \n",
    "                           filled=True, \n",
    "                           special_characters=True, \n",
    "                           rotate=False)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png(filename)\n",
    "img = mpimg.imread(filename)\n",
    "plt.figure(figsize=(100, 200))\n",
    "plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-Visualiziation of the pumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Import for figure size\n",
    "import geopandas as gpd  # Import for geospatial data\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df_1, geometry=gpd.points_from_xy(df_1.longitude, df_1.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Import matplotlib.pyplot for 'rcParams'\n",
    "\n",
    "# Set the figure size\n",
    "plt.rcParams['figure.figsize'] = 30, 20\n",
    "\n",
    "\n",
    "# let's visualize the data\n",
    "gdf = geopandas.GeoDataFrame(df_1, geometry=geopandas.points_from_xy(df_1.longitude, df_1.latitude))\n",
    "\n",
    "functional = gdf.where(gdf['status_group'] == 0)\n",
    "repair = gdf.where(gdf['status_group'] == 2)\n",
    "abandoned = gdf.where(gdf['status_group'] == 1)\n",
    "broken = gdf.where(gdf['status_group'] == 3)\n",
    "\n",
    "\n",
    "\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# We restrict to Africa\n",
    "ax = world[world.continent == 'Africa'].plot(\n",
    "    color='gray', edgecolor='black')\n",
    "\n",
    "ax.scatter(functional['longitude'], functional['latitude'],\n",
    "           c='green',alpha=.5, s=3)\n",
    "\n",
    "ax.scatter(repair['longitude'], repair['latitude'],\n",
    "           c='blue', alpha=.5, s=5)\n",
    "\n",
    "ax.scatter(broken['longitude'], broken['latitude'],\n",
    "           c='red', alpha=.5, s=5)\n",
    "plt.title(\"Map of Pump Distributions, Green-Functional, Blue-Repair, Red-Broken\", fontsize = 25)\n",
    "\n",
    "plt.ylim(-12, 0)\n",
    "plt.xlim(28,41)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['date_recorded'].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['date_recorded'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date_recorded\"] = pd.to_datetime(data['date_recorded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import decision trees classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle = True, random_state=0)\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "predicted_value = model.predict(X_test)\n",
    "print(predicted_value)\n",
    "#%%\n",
    "tree.plot_tree(model)\n",
    "\n",
    "zeroes = 0\n",
    "ones = 0\n",
    "for i in range(0,len(y_train)):\n",
    "    if y_train[i] == 0:\n",
    "        zeroes +=1\n",
    "    else:\n",
    "        ones +=1\n",
    "      \n",
    "print(zeroes)\n",
    "print(ones)\n",
    "\n",
    "val = 1 - ((zeroes/70)*2 + (ones/70)*2)\n",
    "print(\"Gini :-\",val)\n",
    " \n",
    "match = 0\n",
    "UnMatch = 0\n",
    " \n",
    "for i in range(30):\n",
    "    if predicted_value[i] == y_test[i]:\n",
    "        match += 1\n",
    "    else:\n",
    "        UnMatch += 1\n",
    "         \n",
    "accuracy = match/30\n",
    "print(\"Accuracy is: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG BOOST Classifier\n",
    "Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# training XGboost on the training\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "y_pred = classifier.predict(x_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
